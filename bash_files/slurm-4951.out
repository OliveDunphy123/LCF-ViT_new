/mnt/guanabana/raid/home/qinxu/land_cover_fraction/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Using device: cuda

Creating training dataloaders...

Initializing Training Dataset:
Sentinel data path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_normalized/monthly
Ground truth path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_gt

First few unique IDs:
- 2753185_2015-07
- 2753185_2015-08
- 2753185_2015-09
- 2753185_2015-10
- 2753185_2015-11
Found 11340 unique location-time pairs for monthly training data

Initializing Training Dataset:
Sentinel data path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_normalized/yearly
Ground truth path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_gt

First few unique IDs:
- 2753185_2015
- 2753185_2016
- 2753185_2017
- 2753185_2018
- 2753198_2015
Found 1080 unique location-time pairs for yearly training data

Testing Monthly Data:
Monthly Model Input Size: 94500
Monthly Model Output Size: 175

Training Monthly Model:
Epoch 1/5:   0%|          | 0/355 [00:00<?, ?it/s]/mnt/guanabana/raid/home/qinxu/land_cover_fraction/lib/python3.12/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Epoch 1/5:   0%|          | 0/355 [00:32<?, ?it/s]
Error during testing: CUDA out of memory. Tried to allocate 186.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 39.88 MiB is free. Process 3872324 has 22.46 GiB memory in use. Including non-PyTorch memory, this process has 986.00 MiB memory in use. Of the allocated memory 769.29 MiB is allocated by PyTorch, and 10.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
