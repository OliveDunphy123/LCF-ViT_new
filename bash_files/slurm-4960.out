Device 0: NVIDIA TITAN RTX
Using device: cuda

Creating training dataloaders...

Initializing Training Dataset:
Sentinel data path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_normalized/monthly
Ground truth path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_gt

First few unique IDs:
- 2753185_2015-07
- 2753185_2015-08
- 2753185_2015-09
- 2753185_2015-10
- 2753185_2015-11
Found 11340 unique location-time pairs for monthly training data

Initializing Training Dataset:
Sentinel data path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_normalized/yearly
Ground truth path: /mnt/guanabana/raid/shared/dropbox/QinLennart/training_subset_gt

First few unique IDs:
- 2753185_2015
- 2753185_2016
- 2753185_2017
- 2753185_2018
- 2753198_2015
Found 1080 unique location-time pairs for yearly training data

Testing Monthly Data:
Monthly Model Input Size: 94500
Monthly Model Output Size: 175

Training Monthly Model:
Epoch 1/5:   0%|          | 0/355 [00:00<?, ?it/s]Epoch 1/5:   0%|          | 0/355 [00:26<?, ?it/s]
Error during testing: CUDA out of memory. Tried to allocate 186.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 39.88 MiB is free. Process 2605807 has 22.46 GiB memory in use. Including non-PyTorch memory, this process has 986.00 MiB memory in use. Of the allocated memory 769.29 MiB is allocated by PyTorch, and 10.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
